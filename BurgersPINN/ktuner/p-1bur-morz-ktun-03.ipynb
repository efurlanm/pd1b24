{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN 1D-BURGERS MORZARIA KERAS-TUNER TF2 - 03\n",
    "\n",
    "*2024-06-24*\n",
    "\n",
    "- RandomSearch optimization\n",
    "\n",
    "*2024-06-23*\n",
    "\n",
    "- running using LBFG-S\n",
    "\n",
    "References:\n",
    "\n",
    "- Keras Tuner. <https://keras.io/guides/keras_tuner/getting_started>\n",
    "- L-BFGS algorithm. <https://www.tensorflow.org/probability/api_docs/python/tfp/optimizer/lbfgs_minimize>\n",
    "- Deep Morzaria (2020). <https://github.com/deepmorzaria/Physics-Informed-Neural-Network-PINNs---TF-2.0>\n",
    "- Pi-Yueh Chuang (2020). <https://gist.github.com/piyueh/712ec7d4540489aad2dcfb80f9a54993>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# 3456789_123456789_123456789_123456789_123456789_123456789_123456789_123456789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6DK2_TqoYjx",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_CPP_MIN_LOG_LEVEL=3\n"
     ]
    }
   ],
   "source": [
    "%env TF_CPP_MIN_LOG_LEVEL=3\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks, layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten\n",
    "import keras_tuner\n",
    "from scipy.io import loadmat\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot, cm\n",
    "from pyDOE import lhs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tf.keras.backend.set_floatx(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "# NO_OF_HIDD_LAYE = 1  # 6\n",
    "# NEUR_PER_LAYE = 8    # 20\n",
    "NO_MAX_INTE = 100  # 1000\n",
    "NO_OF_INTE_POIN = 1000  # 8000\n",
    "NO_OF_COLL_POIN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "shwtPH4KxKdK"
   },
   "outputs": [],
   "source": [
    "# These contain data for the actual output of PDE at t=0.25,0.50,0.75\n",
    "# which would later used to compare with the predicted output of neural\n",
    "# network\n",
    "actual_outputs_1 = pd.read_excel(DATA_DIR + r\"t=0.25.xlsx\")\n",
    "actual_outputs_2 = pd.read_excel(DATA_DIR + r\"t=0.50.xlsx\")\n",
    "actual_outputs_3 = pd.read_excel(DATA_DIR + r\"t=0.75.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMRGQrCtoyJY"
   },
   "outputs": [],
   "source": [
    "data = loadmat(DATA_DIR + \"burgers_shock.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lI-njAYSpGJs"
   },
   "outputs": [],
   "source": [
    "# Extracting input and outputs from data\n",
    "x = data[\"x\"]  # (256,1)      x varies from [-1,1]\n",
    "t = data[\"t\"]  # (100,1)      t varies from [0,1]\n",
    "Exact = data[\"usol\"].T  # (100,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "816ngyZByxVj"
   },
   "outputs": [],
   "source": [
    "X, T = np.meshgrid(x, t)  # X-(100,256)  T-(100,256)\n",
    "# flatten() will return a list of numbers. [:,None] converts it to\n",
    "# list of list. Refer to numpy.hstack syntax for more details.\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "\n",
    "# X_star (25600,2) Set of points for each time instant\n",
    "# u_star (25600,1) ouputs for each (x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6mUSOFSzae_"
   },
   "outputs": [],
   "source": [
    "# data for initial boundary condition   (256,2)\n",
    "xx1 = np.hstack((X[0:1, :].T, T[0:1, :].T))\n",
    "# data for boundary condition for x=-1  (100,2)\n",
    "xx2 = np.hstack((X[:, 0:1], T[:, 0:1]))\n",
    "# data for boundary condition for x=1   (100,2)\n",
    "xx3 = np.hstack((X[:, -1:], T[:, -1:]))\n",
    "\n",
    "uu1 = Exact[0:1, :].T\n",
    "uu2 = Exact[:, 0:1]\n",
    "uu3 = Exact[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9vduG01T-PT"
   },
   "outputs": [],
   "source": [
    "lb = X_star.min(axis=0)  # (1,2)   [-1 0]\n",
    "ub = X_star.max(axis=0)  # (1,2)   [1 0.99]\n",
    "\n",
    "\n",
    "# (456,2) stacking up all data related to bc's\n",
    "X_u_train_ds = np.vstack([xx1, xx2, xx3])\n",
    "\n",
    "# (8000,2) lhs is used to generate random sample of points.\n",
    "# 2 is no of variables(x,t).\n",
    "X_f_train = lb + (ub - lb) * lhs(2, NO_OF_INTE_POIN)\n",
    "\n",
    "# (NO_OF_INTE_POIN + 456,2)\n",
    "X_f_train = np.vstack((X_f_train, X_u_train_ds))\n",
    "\n",
    "# (456,1) correspoing to X_u_train_ds\n",
    "u_train_ds = np.vstack([uu1, uu2, uu3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQWrvxbhUomX"
   },
   "outputs": [],
   "source": [
    "# random sample of collocation points from 456 numbers\n",
    "idx = np.random.choice(\n",
    "    X_u_train_ds.shape[0], NO_OF_COLL_POIN, replace=False\n",
    ")\n",
    "\n",
    "# Those collocation points chosen from boundary conditions data\n",
    "X_u_train = X_u_train_ds[idx, :]\n",
    "\n",
    "# Output corresponding to collocation points\n",
    "u_train = u_train_ds[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZDxvxZNBOUdI",
    "outputId": "935fcae8-8b28-4966-908b-84e95f5b2227"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2), (456, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_u_train.shape, X_u_train_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZDxvxZNBOUdI",
    "outputId": "935fcae8-8b28-4966-908b-84e95f5b2227"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 1), (456, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train.shape, u_train_ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_u_train_ds[-100:]\n",
    "u_val = u_train_ds[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 2), (100, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape, u_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8goqSxvczUn"
   },
   "outputs": [],
   "source": [
    "# subscript u denotes collocation/boundary points and\n",
    "# subscript f denotes interior points\n",
    "x_u = X_u_train[:, 0:1]  # Separating x,t from X_u_train\n",
    "t_u = X_u_train[:, 1:2]\n",
    "x_f = X_f_train[:, 0:1]\n",
    "t_f = X_f_train[:, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ugRc6RY7if50"
   },
   "outputs": [],
   "source": [
    "# Converting to tensor varialble. Essential for calculating\n",
    "# gradients later on.\n",
    "x_u_tf = tf.Variable(x_u)\n",
    "t_u_tf = tf.Variable(t_u)\n",
    "x_f_tf = tf.Variable(x_f)\n",
    "t_f_tf = tf.Variable(t_f)\n",
    "X_f_train_tf = tf.Variable(X_f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oKsB2_0rqqG"
   },
   "outputs": [],
   "source": [
    "# function for calculating loss wrt to interior points\n",
    "def interior_loss(model):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X_f_train_tf)\n",
    "        with tf.GradientTape() as tape2:\n",
    "            u_predicted = model(X_f_train_tf)\n",
    "        grad = tape2.gradient(u_predicted, X_f_train_tf)\n",
    "        du_dx = grad[:, 0]\n",
    "        du_dt = grad[:, 1]\n",
    "    j = tape.gradient(grad, X_f_train_tf)\n",
    "    d2u_dx2 = j[:, 0]\n",
    "\n",
    "    u_predicted = tf.cast(u_predicted, dtype=tf.float64)\n",
    "    du_dx = tf.reshape(du_dx, [NO_OF_INTE_POIN + 456, 1])\n",
    "    d2u_dx2 = tf.reshape(d2u_dx2, [NO_OF_INTE_POIN + 456, 1])\n",
    "    du_dt = tf.reshape(du_dt, [NO_OF_INTE_POIN + 456, 1])\n",
    "\n",
    "    f = du_dt + u_predicted * du_dx - (0.01 / 3.14 * d2u_dx2)\n",
    "    f = tf.math.reduce_mean(tf.math.square(f))\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LBFG-S, which is second order optimizer, has been used to update the weights and biases because conventional first order optimizers Adam, Gradient descent and RMSprop \n",
    "are slow to converge.LBFG-S is not available by default in Tensorflow 2.0 and hence a function from Tensorflow Probability has been used. This has not been coded by me\n",
    "except for a minor addition to loss function(check the loss_value variable). Please refer to the link below to get a better idea.\n",
    "\n",
    "<https://gist.github.com/piyueh/712ec7d4540489aad2dcfb80f9a54993>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qf-rM9wQUPUC"
   },
   "outputs": [],
   "source": [
    "def function_factory(model, loss, train_x, train_y):\n",
    "    \"\"\"A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    Args:\n",
    "        model [in]: an instance of `tf.keras.Model` or its subclasses.\n",
    "        loss [in]: a function with signature loss_value = loss(pred_y, true_y).\n",
    "        train_x [in]: the input part of training data.\n",
    "        train_y [in]: the output part of training data.\n",
    "    Returns:\n",
    "        A function that has a signature of:\n",
    "            loss_value, gradients = f(model_parameters).\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain the shapes of all trainable parameters in the model\n",
    "    shapes = tf.shape_n(model.trainable_variables)\n",
    "    n_tensors = len(shapes)\n",
    "\n",
    "    # we'll use tf.dynamic_stitch and tf.dynamic_partition later,\n",
    "    # so we need to prepare required information first\n",
    "    count = 0\n",
    "    idx = []  # stitch indices\n",
    "    part = []  # partition indices\n",
    "\n",
    "    for i, shape in enumerate(shapes):\n",
    "        n = np.prod(shape)\n",
    "        idx.append(\n",
    "            tf.reshape(\n",
    "                tf.range(count, count + n, dtype=tf.int32), shape\n",
    "            )\n",
    "        )\n",
    "        part.extend([i] * n)\n",
    "        count += n\n",
    "\n",
    "    part = tf.constant(part)\n",
    "\n",
    "    # =======================================\n",
    "    @tf.function\n",
    "    def assign_new_model_parameters(params_1d):\n",
    "        \"\"\"A function updating the model's parameters with a 1D tf.Tensor.\n",
    "        Args:\n",
    "            params_1d [in]: a 1D tf.Tensor representing the model's trainable parameters.\n",
    "        \"\"\"\n",
    "        params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            model.trainable_variables[i].assign(\n",
    "                tf.reshape(param, shape)\n",
    "            )\n",
    "        return\n",
    "        # ---------------------------------------\n",
    "\n",
    "    # =======================================\n",
    "    # create a function that will be returned by this factory\n",
    "    @tf.function\n",
    "    def f(params_1d):\n",
    "        \"\"\"A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "        This function is created by function_factory.\n",
    "        Args:\n",
    "           params_1d [in]: a 1D tf.Tensor.\n",
    "        Returns:\n",
    "            A scalar loss and the gradients w.r.t. the `params_1d`.\n",
    "        \"\"\"\n",
    "\n",
    "        # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
    "        with tf.GradientTape() as tape:\n",
    "            # update the parameters in the model\n",
    "            assign_new_model_parameters(params_1d)\n",
    "            # calculate the loss\n",
    "            loss_value = loss(model(train_x, training=True), train_y)\n",
    "            int_loss = interior_loss(model)\n",
    "            loss_value = loss_value + int_loss\n",
    "\n",
    "        # calculate gradients and convert to 1D tf.Tensor\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        grads = tf.dynamic_stitch(idx, grads)\n",
    "\n",
    "        # print out iteration & loss\n",
    "        f.iter.assign_add(1)\n",
    "        tf.print(\"Iter:\", f.iter, \"loss:\", loss_value)\n",
    "\n",
    "        # store loss value so we can retrieve later\n",
    "        tf.py_function(f.history.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "        # training_path.append(model.get_weights())\n",
    "\n",
    "        return loss_value, grads\n",
    "        # ---------------------------------------\n",
    "\n",
    "    # store these information as members so we can use them\n",
    "    # outside the scope\n",
    "    f.iter = tf.Variable(0)\n",
    "    f.idx = idx\n",
    "    f.part = part\n",
    "    f.shapes = shapes\n",
    "    f.assign_new_model_parameters = assign_new_model_parameters\n",
    "    f.history = []\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TqqClyWzq6Mx",
    "outputId": "6eb62b25-3cdf-4e3e-94be-7795954377fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def KERAS_CODE(units, activation, lr, nlayers):\n",
    "    global model\n",
    "    model = Sequential()\n",
    "    # INPUT LAYER\n",
    "    model.add(keras.Input(shape=(2,)))\n",
    "    # model.add(Flatten())\n",
    "    # model.add(Dense(units=units))\n",
    "    # , name=\"input_layer\"\n",
    "    # activation=\"tanh\",\n",
    "    # kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "\n",
    "    # HIDDEN LAYERS\n",
    "    for i in range(nlayers):\n",
    "        model.add(Dense(units=units, activation=activation))\n",
    "        # , name=\"hidden_\" + str(i + 1)\n",
    "        # kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "\n",
    "    # OUTPUT LAYER\n",
    "    model.add(Dense(1, activation=None))\n",
    "    # , name=\"output_layer\"\n",
    "    # activation=\"softmax\"\n",
    "\n",
    "    # COMPILE\n",
    "    # model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "    #               loss=\"mse\",\n",
    "    #               metrics=[\"accuracy\"])\n",
    "\n",
    "    funcfac = function_factory(\n",
    "        model,\n",
    "        tf.keras.losses.MeanSquaredError(),\n",
    "        X_u_train,\n",
    "        u_train,\n",
    "    )\n",
    "\n",
    "    init_params = tf.dynamic_stitch(\n",
    "        funcfac.idx, model.trainable_variables\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    results = tfp.optimizer.lbfgs_minimize(\n",
    "        value_and_gradients_function=funcfac,\n",
    "        initial_position=init_params,\n",
    "        max_iterations=NO_MAX_INTE,\n",
    "        parallel_iterations=4,\n",
    "    )\n",
    "    min = results.objective_value.numpy()\n",
    "\n",
    "    funcfac.assign_new_model_parameters(results.position)  \n",
    "    \n",
    "    return min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTuner(keras_tuner.RandomSearch):\n",
    "\n",
    "    def run_trial(self, trial, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "        # KERAS_CODE(units, activation, nlayers, lr)\n",
    "        rtn = KERAS_CODE(\n",
    "            units = hp.Int(\"units\", 1, 32, 2),\n",
    "            activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            nlayers = hp.Int(\"nlayers\", 1, 16, 2),\n",
    "            lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\"),\n",
    "        )\n",
    "        return rtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = MyTuner(\n",
    "    max_trials=10,\n",
    "    overwrite=True,\n",
    "    directory=\"keras-tuner\",\n",
    "    project_name=\"test01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 01s]\n",
      "default_objective: 0.2846603324586185\n",
      "\n",
      "Best default_objective So Far: 0.08405156076471121\n",
      "Total elapsed time: 00h 01m 29s\n"
     ]
    }
   ],
   "source": [
    "tuner.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras-tuner/test01\n",
      "Showing 10 best trials\n",
      "Objective(name=\"default_objective\", direction=\"min\")\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "units: 3\n",
      "activation: tanh\n",
      "nlayers: 3\n",
      "lr: 0.0002772798879792949\n",
      "Score: 0.08405156076471121\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units: 7\n",
      "activation: tanh\n",
      "nlayers: 1\n",
      "lr: 0.00010285438182890766\n",
      "Score: 0.10669765120572944\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units: 5\n",
      "activation: tanh\n",
      "nlayers: 13\n",
      "lr: 0.0029185317760266416\n",
      "Score: 0.11482401855464935\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units: 5\n",
      "activation: relu\n",
      "nlayers: 1\n",
      "lr: 0.00014281301445455595\n",
      "Score: 0.13327633178207954\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units: 31\n",
      "activation: relu\n",
      "nlayers: 1\n",
      "lr: 0.0030039157763138155\n",
      "Score: 0.1343622365939171\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "units: 19\n",
      "activation: relu\n",
      "nlayers: 3\n",
      "lr: 0.005408347666390224\n",
      "Score: 0.1372943387803699\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "units: 1\n",
      "activation: tanh\n",
      "nlayers: 15\n",
      "lr: 0.00027524586095343357\n",
      "Score: 0.1506364764520794\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units: 13\n",
      "activation: relu\n",
      "nlayers: 9\n",
      "lr: 0.0010472536977192324\n",
      "Score: 0.1776327881045462\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units: 1\n",
      "activation: relu\n",
      "nlayers: 1\n",
      "lr: 0.0001\n",
      "Score: 0.2846603324586185\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "units: 1\n",
      "activation: relu\n",
      "nlayers: 7\n",
      "lr: 0.0003190483519213392\n",
      "Score: 0.2846603324586185\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Execution from this point forward only makes sense if retrained using the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6ynCyrXJJ3M"
   },
   "outputs": [],
   "source": [
    "p = np.vstack([X_f_train, X_u_train])\n",
    "q = np.vstack([model.predict(X_f_train), model.predict(X_u_train)])[\n",
    "    :, 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_helper(inputs, outputs, title):\n",
    "    pyplot.figure(figsize=(8, 4))\n",
    "    pyplot.tricontourf(\n",
    "        inputs[:, 1], inputs[:, 0], outputs.flatten(), 100\n",
    "    )\n",
    "    pyplot.scatter(\n",
    "        X_u_train[:, 1], X_u_train[:, 0], marker=\"x\", s=100, c=\"k\"\n",
    "    )\n",
    "    pyplot.xlabel(\"t\")\n",
    "    pyplot.ylabel(\"x\")\n",
    "    pyplot.title(title)\n",
    "    pyplot.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "2MqTYJmU9Pnu",
    "outputId": "dcbd250e-6ae2-4aa2-9e31-466784288a55"
   },
   "outputs": [],
   "source": [
    "plot_helper(p, q, \"u (x,t)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wMeFp_pG938v"
   },
   "outputs": [],
   "source": [
    "x_t_25 = np.hstack((X[0:1, :].T, T[25:26, :].T))\n",
    "x_t_50 = np.hstack((X[0:1, :].T, T[50:51, :].T))\n",
    "x_t_75 = np.hstack((X[0:1, :].T, T[75:76, :].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "ft8mgFduz5nh",
    "outputId": "e416e21c-fe48-4408-aafe-812d2c67f5e9"
   },
   "outputs": [],
   "source": [
    "fig, axs = pyplot.subplots(1, 3, figsize=(8, 4), sharey=True)\n",
    "(l1,) = axs[0].plot(\n",
    "    actual_outputs_1[\"x\"], actual_outputs_1[\"t\"], linewidth=6, color=\"b\"\n",
    ")\n",
    "(l2,) = axs[0].plot(\n",
    "    x_t_25[:, 0],\n",
    "    model.predict(x_t_25),\n",
    "    linewidth=6,\n",
    "    linestyle=\"dashed\",\n",
    "    color=\"r\",\n",
    ")\n",
    "axs[0].set_title(\"t=0.25\")\n",
    "axs[0].set_xlabel(\"x\")\n",
    "axs[0].set_ylabel(\"u (x,t)\")\n",
    "\n",
    "axs[1].plot(\n",
    "    actual_outputs_2[\"x\"], actual_outputs_2[\"t\"], linewidth=6, color=\"b\"\n",
    ")\n",
    "axs[1].plot(\n",
    "    x_t_50[:, 0],\n",
    "    model.predict(x_t_50),\n",
    "    linewidth=6,\n",
    "    linestyle=\"dashed\",\n",
    "    color=\"r\",\n",
    ")\n",
    "axs[1].set_title(\"t=0.50\")\n",
    "axs[1].set_xlabel(\"x\")\n",
    "\n",
    "axs[2].plot(\n",
    "    actual_outputs_3[\"x\"], actual_outputs_3[\"t\"], linewidth=6, color=\"b\"\n",
    ")\n",
    "axs[2].plot(\n",
    "    x_t_75[:, 0],\n",
    "    model.predict(x_t_75),\n",
    "    linewidth=6,\n",
    "    linestyle=\"dashed\",\n",
    "    color=\"r\",\n",
    ")\n",
    "axs[2].set_title(\"t=0.75\")\n",
    "axs[2].set_xlabel(\"x\")\n",
    "\n",
    "# line_labels = ['Exact','Predicted']\n",
    "\n",
    "fig.legend(\n",
    "    handles=(l1, l2), labels=(\"Exact\", \"Predicted\"), loc=\"upper right\"\n",
    ")\n",
    "# pyplot.savefig('graphs3.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i_lLBGcyArQD"
   },
   "outputs": [],
   "source": [
    "r = model.predict(X_u_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "j0njjtHGUUMn",
    "outputId": "13b5ab37-9b26-4fa7-bdd3-6a688c49348f"
   },
   "outputs": [],
   "source": [
    "error_u = np.linalg.norm(u_train - r, 2) / np.linalg.norm(u_star, 2)\n",
    "print(error_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oZ97PCylMKnY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNLCvytOl/uCB2sxe/8/HsJ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "PINNs_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
